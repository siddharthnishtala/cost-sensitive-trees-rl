{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from utils import load_yaml, load_pickled_object\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "mpl.rc(\"savefig\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "results_dir = \"results\"\n",
    "analysis_dir = \"analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algo_results(algo, results_dir):\n",
    "\n",
    "    columns = [\n",
    "        \"env\", \"algo\", \"seed\", \n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        \"mean_episode_reward\", \"std_episode_reward\", \n",
    "        \"mean_fidelity\", \"std_fidelity\", \n",
    "        \"mean_misclassification_cost\", \"std_misclassification_cost\"\n",
    "    ]\n",
    "    if algo in [\"VIPER\", \"CS-VIPER\"]:\n",
    "        columns += [\"dt_depth\"]\n",
    "        columns += metrics\n",
    "    else:\n",
    "        columns += [\"no_of_experts\", \"dt_depth\"]\n",
    "        columns += metrics\n",
    "        columns += [\n",
    "            \"mean_episode_reward_d\", \"std_episode_reward_d\", \n",
    "            \"mean_fidelity_d\", \"std_fidelity_d\", \n",
    "            \"mean_misclassification_cost_d\", \"std_misclassification_cost_d\"\n",
    "        ]  \n",
    "\n",
    "    algo_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "    algo_dir = os.path.join(results_dir, algo.replace(\"-\", \"_\").lower())\n",
    "    for subdir in os.listdir(algo_dir):\n",
    "\n",
    "        subdir_tags = subdir.split(\"_\")\n",
    "        if algo in [\"VIPER\", \"CS-VIPER\"]:\n",
    "            row = {\"env\": subdir_tags[0], \"algo\": algo, \"dt_depth\": int(subdir_tags[1]), \"seed\": int(subdir_tags[2])}\n",
    "        else:\n",
    "            row = {\"env\": subdir_tags[0], \"algo\": algo, \"no_of_experts\": int(subdir_tags[1]), \"dt_depth\": int(subdir_tags[2]), \"seed\": int(subdir_tags[3])}\n",
    "            results = load_yaml(os.path.join(algo_dir, subdir, \"results_d.yml\"))\n",
    "            for metric in metrics:\n",
    "                row[metric + \"_d\"] = results[metric + \"_d\"]\n",
    "\n",
    "        results = load_yaml(os.path.join(algo_dir, subdir, \"results.yml\"))\n",
    "        for metric in metrics:\n",
    "            row[metric] = results[metric]\n",
    "\n",
    "        algo_results = algo_results.append(row, ignore_index = True)\n",
    "\n",
    "    algo_results.sort_values(by=[\"env\", \"dt_depth\", \"seed\"], inplace=True)\n",
    "\n",
    "    return algo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_comparison_plots(algo, results_dir, analysis_dir, env=None):\n",
    "\n",
    "    if algo == \"VIPER\":\n",
    "        algo1 = \"VIPER\"\n",
    "        algo2 = \"CS-VIPER\"\n",
    "        dir_name = \"VIPER vs CS-VIPER\"\n",
    "    else:\n",
    "        algo1 = \"MoET\"\n",
    "        algo2 = \"CS-MoET\"\n",
    "        dir_name = \"MoET vs CS-MoET\"\n",
    "\n",
    "    if not os.path.exists(os.path.join(analysis_dir, dir_name)):\n",
    "        os.makedirs(os.path.join(analysis_dir, dir_name))\n",
    "\n",
    "    algo1_results = get_algo_results(algo1, results_dir)\n",
    "    algo2_results = get_algo_results(algo2, results_dir)\n",
    "\n",
    "    if env:\n",
    "        algo1_results = algo1_results.loc[algo1_results[\"env\"]==env]\n",
    "        algo2_results = algo2_results.loc[algo2_results[\"env\"]==env]\n",
    "\n",
    "    results = pd.concat([algo1_results, algo2_results], axis=0)\n",
    "\n",
    "    for env in results[\"env\"].unique():\n",
    "\n",
    "        env_results = results.loc[results[\"env\"] == env]\n",
    "\n",
    "        metrics = [\"mean_episode_reward\", \"mean_fidelity\", \"mean_misclassification_cost\"]\n",
    "        metrics_l = [\"Cumulative Reward\", \"Fidelity\", \"Misclassification Cost\"]\n",
    "\n",
    "        os.makedirs(os.path.join(analysis_dir, dir_name, env), exist_ok=True)\n",
    "\n",
    "        if \"no_of_experts\" not in results.columns:\n",
    "            \n",
    "            for i in range(len(metrics)):\n",
    "                \n",
    "                fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "                g = sns.barplot(data=env_results, x=\"dt_depth\", y=metrics[i], hue=\"algo\", errorbar=\"sd\", errwidth=2, ax=ax)\n",
    "\n",
    "                ax.set_xlabel(\"Tree Depth\")    \n",
    "                ax.set_ylabel(metrics_l[i])    \n",
    "\n",
    "                if metrics_l[i] == \"Misclassification Cost\":\n",
    "                    ax.legend(loc='upper right')\n",
    "                else:\n",
    "                    ax.legend(loc='lower right')\n",
    "\n",
    "                ax.set_title(env)\n",
    "                plt.tight_layout(pad=0.5)\n",
    "                fig.savefig(os.path.join(analysis_dir, dir_name, env, metrics_l[i] + \".png\"))\n",
    "\n",
    "        else:\n",
    "\n",
    "            metrics += [\"mean_episode_reward_d\", \"mean_fidelity_d\", \"mean_misclassification_cost_d\"]\n",
    "            metrics_l += [\"Cumulative Reward (disc.)\", \"Fidelity (disc.)\", \"Misclassification Cost (disc.)\"]\n",
    "\n",
    "            for no_of_experts in results[\"no_of_experts\"].unique():\n",
    "\n",
    "                os.makedirs(os.path.join(analysis_dir, dir_name, env, str(no_of_experts)), exist_ok=True)\n",
    "\n",
    "                env_exp_results = env_results.loc[env_results[\"no_of_experts\"]==no_of_experts]\n",
    "\n",
    "                for i in range(len(metrics)):\n",
    "                \n",
    "                    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "                    g = sns.barplot(data=env_exp_results, x=\"dt_depth\", y=metrics[i], hue=\"algo\", errorbar=\"sd\", errwidth=2, ax=ax)\n",
    "\n",
    "                    ax.set_xlabel(\"Tree Depth\")    \n",
    "                    ax.set_ylabel(metrics_l[i])    \n",
    "\n",
    "                    if metrics_l[i] == \"Misclassification Cost\":\n",
    "                        ax.legend(loc='upper right')\n",
    "                    else:\n",
    "                        ax.legend(loc='lower right')\n",
    "\n",
    "                    ax.set_title(env + \": \" + str(no_of_experts) + \" Experts\")\n",
    "                    plt.tight_layout(pad=0.5)\n",
    "                    fig.savefig(os.path.join(analysis_dir, dir_name, env, str(no_of_experts), metrics_l[i] + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_moet_performance_comparison_plots(no_of_experts, results_dir, analysis_dir, env=None):\n",
    "\n",
    "    algo1 = \"MoET\"\n",
    "    algo2 = \"CS-MoET\"\n",
    "    dir_name = \"MoET vs CS-MoET\"\n",
    "\n",
    "    if not os.path.exists(os.path.join(analysis_dir, dir_name)):\n",
    "        os.makedirs(os.path.join(analysis_dir, dir_name))\n",
    "\n",
    "    algo1_results = get_algo_results(algo1, results_dir)\n",
    "    algo2_results = get_algo_results(algo2, results_dir)\n",
    "\n",
    "    if env:\n",
    "        algo1_results = algo1_results.loc[algo1_results[\"env\"]==env]\n",
    "        algo2_results = algo2_results.loc[algo2_results[\"env\"]==env]\n",
    "\n",
    "    results = pd.concat([algo1_results, algo2_results], axis=0)\n",
    "    results = results.loc[results[\"no_of_experts\"].isin(no_of_experts)]\n",
    "    results[\"algo\"] = results[\"algo\"] + \"-\" + results[\"no_of_experts\"].astype(str)\n",
    "\n",
    "    for env in results[\"env\"].unique():\n",
    "\n",
    "        env_results = results.loc[results[\"env\"] == env]\n",
    "\n",
    "        metrics = [\"mean_episode_reward\", \"mean_fidelity\", \"mean_misclassification_cost\"]\n",
    "        metrics_l = [\"Cumulative Reward\", \"Fidelity\", \"Misclassification Cost\"]\n",
    "\n",
    "        os.makedirs(os.path.join(analysis_dir, dir_name, env), exist_ok=True)\n",
    "            \n",
    "        for i in range(len(metrics)):\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "            g = sns.barplot(data=env_results, x=\"dt_depth\", y=metrics[i], hue=\"algo\", hue_order=[\"MoET-2\", \"CS-MoET-2\", \"MoET-3\", \"CS-MoET-3\"], palette=[\"tab:blue\" , \"tab:orange\", \"tab:blue\" , \"tab:orange\"], errorbar=\"sd\", errwidth=2, ax=ax)\n",
    "\n",
    "            hatches = itertools.cycle(['..', '..', '//', '//'])\n",
    "            for k, bar in enumerate(ax.patches):\n",
    "                if k % 15 == 0:\n",
    "                    hatch = next(hatches)\n",
    "                bar.set_hatch(hatch)\n",
    "\n",
    "            ax.set_xlabel(\"Tree Depth\")    \n",
    "            ax.set_ylabel(metrics_l[i])    \n",
    "\n",
    "            if metrics_l[i] == \"Misclassification Cost\":\n",
    "                ax.legend(loc='upper right')\n",
    "            else:\n",
    "                ax.legend(loc='lower right')\n",
    "\n",
    "            ax.set_title(env)\n",
    "            plt.tight_layout(pad=0.5)\n",
    "            fig.savefig(os.path.join(analysis_dir, dir_name, env, metrics_l[i] + \".png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_moet_performance_comparison_plots([2, 3], results_dir, analysis_dir, env=\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_moet_performance_comparison_plots([2, 3], results_dir, analysis_dir, env=\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_moet_performance_comparison_plots([2, 3], results_dir, analysis_dir, env=\"FourRooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_moet_performance_comparison_plots([2, 3], results_dir, analysis_dir, env=\"highway-fast-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_performance_comparison_plots(\"VIPER\", results_dir, analysis_dir, env=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_performance_comparison_plots(\"MoET\", results_dir, analysis_dir, env=\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_performance_comparison_plots(\"MoET\", results_dir, analysis_dir, env=\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_performance_comparison_plots(\"MoET\", results_dir, analysis_dir, env=\"FourRooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_performance_comparison_plots(\"MoET\", results_dir, analysis_dir, env=\"highway-fast-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algo_models(algo, results_dir, env, dt_depth, no_of_experts=None):\n",
    "\n",
    "    if algo in [\"VIPER\", \"CS-VIPER\"]:\n",
    "        model_name = \"tree.pkl\"\n",
    "        regex_pattern = env + \"_\" + str(dt_depth) + \"_\"\n",
    "    else:\n",
    "        model_name = \"model.pkl\"\n",
    "        regex_pattern = env + \"_\" + str(no_of_experts) + \"_\" + str(dt_depth) + \"_\"\n",
    "\n",
    "    algo_results_dirs = os.listdir(os.path.join(results_dir, algo.replace(\"-\", \"_\").lower()))\n",
    "\n",
    "    pattern = re.compile(regex_pattern)\n",
    "\n",
    "    algo_results_dirs = [x for x in algo_results_dirs if pattern.match(x)]\n",
    "\n",
    "    algo_model_paths = [os.path.join(results_dir, algo.replace(\"-\", \"_\").lower(), x, model_name) for x in algo_results_dirs]\n",
    "\n",
    "    algo_models = [load_pickled_object(path) for path in algo_model_paths]\n",
    "\n",
    "    return algo_models\n",
    "\n",
    "def generate_misclassification_cost_ecdf_plots(algo, results_dir, analysis_dir, env, dt_depth, no_of_experts=None, xlims=None):\n",
    "\n",
    "    if algo == \"VIPER\":\n",
    "        algo1 = \"VIPER\"\n",
    "        algo2 = \"CS-VIPER\"\n",
    "        dir_name = \"VIPER vs CS-VIPER\"\n",
    "    else:\n",
    "        algo1 = \"MoET\"\n",
    "        algo2 = \"CS-MoET\"\n",
    "        dir_name = \"MoET vs CS-MoET\"\n",
    "\n",
    "    if not os.path.exists(os.path.join(analysis_dir, dir_name)):\n",
    "        os.makedirs(os.path.join(analysis_dir, dir_name))\n",
    "\n",
    "    algo1_models = get_algo_models(algo1, results_dir, env, dt_depth, no_of_experts)\n",
    "    algo2_models = get_algo_models(algo2, results_dir, env, dt_depth, no_of_experts)\n",
    "\n",
    "    dataset = load_pickled_object(os.path.join(\"datasets\", env, \"2000.pkl\"))\n",
    "    obss, acts, qs = dataset\n",
    "\n",
    "    states = np.array(obss)\n",
    "    qvals = np.array([q[0, :] for q in qs])\n",
    "\n",
    "    algo1_model_preds = [model.predict(states) for model in algo1_models]\n",
    "    algo2_model_preds = [model.predict(states) for model in algo2_models]\n",
    "\n",
    "    algo1_model_qvals = [qvals[np.arange(qvals.shape[0]), algo1_model_pred] for algo1_model_pred in algo1_model_preds]\n",
    "    algo2_model_qvals = [qvals[np.arange(qvals.shape[0]), algo2_model_pred] for algo2_model_pred in algo2_model_preds]\n",
    "\n",
    "    dfs = []\n",
    "    for i in range(len(algo1_models)):\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Max Q Value\"] = np.max(qvals, axis=1)\n",
    "        df[\"Q Value\"] = algo1_model_qvals[i]\n",
    "        df[\"Algorithm\"] = algo1\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    algo1_df = pd.concat(dfs, axis=0)\n",
    "    algo1_df[\"Misclassification Cost\"] = algo1_df[\"Max Q Value\"] - algo1_df[\"Q Value\"]\n",
    "\n",
    "    # algo1_accuracy = round((algo1_df[algo1_df[\"Misclassification Cost\"] == 0].shape[0]/algo1_df.shape[0])*100, 2)\n",
    "\n",
    "    dfs = []\n",
    "    for i in range(len(algo2_models)):\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Max Q Value\"] = np.max(qvals, axis=1)\n",
    "        df[\"Q Value\"] = algo2_model_qvals[i]\n",
    "        df[\"Algorithm\"] = algo2\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    algo2_df = pd.concat(dfs, axis=0)\n",
    "    algo2_df[\"Misclassification Cost\"] = algo2_df[\"Max Q Value\"] - algo2_df[\"Q Value\"]\n",
    "\n",
    "    # algo2_accuracy = round((algo2_df[algo2_df[\"Misclassification Cost\"] == 0].shape[0]/algo2_df.shape[0])*100, 2)\n",
    "\n",
    "    df = pd.concat([algo1_df, algo2_df], axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "    sns.ecdfplot(df[df[\"Misclassification Cost\"] > 0], x=\"Misclassification Cost\", hue=\"Algorithm\", hue_order=[algo1, algo2], ax=ax)\n",
    "\n",
    "    # legend = ax.get_legend()\n",
    "    # new_labels = [algo1 + \" (\" + str(algo1_accuracy) + \"%)\", algo2 + \" (\" + str(algo2_accuracy) + \"%)\"]\n",
    "    # for t, l in zip(legend.texts, new_labels):\n",
    "    #     t.set_text(l)\n",
    "\n",
    "    sns.move_legend(ax, loc=\"lower right\", title=None)\n",
    "\n",
    "    if xlims:\n",
    "        ax.set(xlim=xlims)\n",
    "    \n",
    "    save_dir = os.path.join(analysis_dir, dir_name, env, \"misclassification_cost_ecdf_plots\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if algo == \"VIPER\":\n",
    "        ax.set(title=env + \": Misclassification Cost ECDF\" + \"; d=\" + str(dt_depth))\n",
    "        fig.tight_layout(pad=0.5)\n",
    "        fig.savefig(os.path.join(save_dir, \"depth=\" + str(dt_depth) + \".png\"))\n",
    "    else:\n",
    "        ax.set(title=env + \": Misclassification Cost ECDF\" + \"; noe=\" + str(no_of_experts) + \", d=\" + str(dt_depth))\n",
    "        fig.tight_layout(pad=0.5)\n",
    "        fig.savefig(os.path.join(save_dir, \"no_of_experts=\" + str(no_of_experts) + \"; depth=\" + str(dt_depth) + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 21):\n",
    "    generate_misclassification_cost_ecdf_plots(\"VIPER\", results_dir, analysis_dir, \"FourRooms\", i, no_of_experts=None, xlims=(0, 0.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    generate_misclassification_cost_ecdf_plots(\"VIPER\", results_dir, analysis_dir, \"highway-fast-v0\", i, no_of_experts=None, xlims=(0, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    generate_misclassification_cost_ecdf_plots(\"VIPER\", results_dir, analysis_dir, \"LunarLander-v2\", i, no_of_experts=None, xlims=(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    generate_misclassification_cost_ecdf_plots(\"VIPER\", results_dir, analysis_dir, \"Taxi-v3\", i, no_of_experts=None, xlims=(0, 12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
